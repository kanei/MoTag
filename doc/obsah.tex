%=========================================================================
% (c) Michal Bidlo, Bohuslav Køena, 2008

\chapter{Úvod}
Tato práce pojednává o vyu¾ití existujících øe¹ení, vyu¾ívajících algoritmù automatického poèítaèového uèení (Conditional Random Fields s vyu¾ítím algoritmù Limited-memory BFGS a Stochastic Gradient Descent) a porovnání výsledkù s existujícími øe¹eními, kterými v na¹em pøípadì je program Morèe vyvíjený na pra¾ské Univerzitì Karlovì, pou¾ívající Hidden Markov Model. Z existujících øe¹ení poté byly vybrány programy CRF++ a CrfSuite.

V dokumentu bude nejdøíve popsáno samotné znaèkování èe¹tiny a jeho pravidla, a potí¾e, které jsou s ním spojeny v porovnání s jinými, jednodu¹¹ími jazyky. Dále budou rozebrány matematické modely a algoritmy, které jsou pou¾ity pøi procesu uèení. K celému procesu bylo potøeba vytvoøit nìkolik nástrojù, buï konvertujících vstupní data pro jednotlivé programy, nebo ulehèujících vlastní proces testování. Mezi tyto patøí skritpy napsané v jazyce Python a bash a informaèní systém, starající se o správu bì¾ících a dokonèených testù a informací o nich, implementovaný za pomocí PHP, SQL a HTML, postavený na open-source CMS systému Drupal. Ve¹kerý kód byl spravován pomocí verzovacího systému GIT a je veøejnì dostupný na internetu\cite{zdrojak}.

Nejdùle¾itìj¹í èástí dokumentù pak jsou výsledky testování, zpracovávaného na ¹kolních strojích pøes vzdálený pøistup. Testy byly provedeny z mnoha rùzných úhlù pohledu na efektivitu a jsou doplnìny o tabulky a grafy. 

Pro úschovu a verzování zdrojového kódu byl vyu¾it program git a kód je veøejnì pøístupný \cite{zdrojak}.

\section{Termíny}
Pro dal¹í text definujeme následující termíny:
\begin{itemize}
	\item \textbf{Znaèka} -- \textit{angl. Label} -- øetìzec popisující vlastnost
 		daného prvku, v na¹em pøípadì urèení mluvnických tøíd.
	\item \textbf{Rys} -- \textit{angl. Feature} -- prvek pou¾ívaný pøi trénování,
		specifikující vlastnost daného prvku ve vstupních datech.
	\item \textbf{Vlastnost} -- \textit{angl. Atribut} -- pravidlo pro generování
		jednotlivých rysù 
	\item \textbf{Lemma} -- základní podoba slova (slovníkový tvar)
\end{itemize}
\chapter{Znaèkování èeského jazyka}
\section{Proè znaèkovat jazyk}
V èeském jazyce má mnoho slov tzv. homonymní tvar -- jedno slovo mù¾e být v rùzných pøípadech naprosto odli¹ným slovním druhem, nebo mít odli¹né urèení slovních kategorií. Pøi strojové analýze textu, pøekladech apod. je proto dùle¾ité, abychom byli schopni tyto kategorie rozli¹ovat a pøiøazovat jednotlivým slovùm jejich správnou znaèku.
\section{Co je to znaèkování}
Proces, pøi kterém je slovu, pøiøazena  znaèka obsahující jeho popis z mluvnického hlediska. Tento proces je èasto rozdìlen na nìkolik krokù, kdy slovu nejdøíve pøiøadíme v¹echny mo¾né znaèky a poté teprve vybereme tu správnou. V této práci se budeme zabývat pouze druhým krokem a v¹echna vstupní data proto ji¾ budou mít nastavena mno¾inu v¹ech znaèek, pøipadajících v úvahu. 
\section{Kategorie popisované znaèkou v èeském jazyce}
Ka¾dé slovo èeského jazyka má mnoho kategorií, do kterých mù¾e být zaøazeno -- od slovního druhu po slovesný vid. V poèítaèovém zpracování jazyka lze pou¾ít nìkolika rùzných pøístupù k ulo¾ení tìchto informací. V na¹em pøípadì bude struktura znaèek odpovídat systému\cite{znacky} pou¾ívanému na pra¾ské Univerzitì Karlovì. Tento systém se skládá ze 16 pozic viz tabulka ~\ref{tab:PopisZnacek}.
\begin{center}
	\begin{tabular}{|c|l|}
		\hline
		Pozice & Popis \\
		\hline
		1 & Slovní druh \\
		2 & Detail slovního druhu \\
		3 & Jmenný rod \\
		4 & Èíslo \\
		5 & Pád \\
		6 & Pøivlastòovací rod \\
		7 & Pøivlastòovací èíslo \\
		8 & Osoba \\
		9 & Èas \\
		10 & Stupeò \\
		11 & Negace \\
		12 & Aktívum/pasívum \\	
		13 & Nepou¾ito \\
		14 & Nepou¾ito \\
		15 & Varianta, stylový pøíznak a pod. \\
		16 & Slovesný vid \\
		\hline
	\end{tabular}
	\captionof{table}{Popis jednotlivých pozic morfologické znaèky}
	\label{tab:PopisZnacek}
\end{center}

\section{Problémy pøi znaèkování}
Zatímco v angliètinì pøi pouhém oznaèkování slov s jednoznaènou znaèkou dosáhneme úspì¹nosti kolem 90\%, v èeském jazyce je situace mnohem slo¾itìj¹í. Kvùli vý¹e zmínìné homonymii a mno¾ství kategorii, do kterých v èeském jazyce slova mù¾eme zaøadit, bychom se v na¹em pøípadì k 90\% urèitì ani nepøiblí¾ili. V angliètinì se proto také pou¾ívá pouze tøí-pozicová znaèka, která velmi ulehèuje práci ve¹kerým automatickým programùm. Pøi takto nízkém poètu je také velmi nízká pamì»ová nároènost a poèet mo¾ných pøechodù není tak vysoký. Naproti tomu v èe¹tinì je kvùli 16 pozicím mo¾no získat celkový poèet znaèek v øádu tisícù a tím se neúmìrnì zvy¹uje èasová i pamì»ová nároènost celého procesu znaèkování. Z tohoto dùvodu jsem se pokou¹el tuto nároènost sní¾it vypu¹tìním ménì dùle¾itých kategorií a tím sní¾it celkový poèet variant k èíslu nepøevy¹ujícímu jeden tisíc. Nároènost na procesorový èas se tím razantnì sní¾ila a dala mi tak ¹anci otestovat mnohem vìt¹í mno¾ství kombinací rysù a nastavení programù.
\section{Data}
K trénování a následné evaluaci výsledkù máme k dispozici okolo 77 000 vìt. Data jsou získána z korpusu PDT 2.0 a ulo¾ena ve formátu dat CSTS\cite{csts}. Pomocí skriptù napsaných pøevá¾nì v jazyce python jsou poté tato data pøevedena do formátu kompatibilního s pou¾itými programy.
\section{Existující øe¹ení pro znaèkování èeského jazyka}
Pro porovnání výsledkù tohoto projektu byl pou¾it program Morèe\cite{Morce} vytvoøený na Ústavu formální a aplikované lingvistiky na Univerzitì Karlovì v Praze. K trénování je vyu¾íván Skrytý Markovùv Model s pou¾itím Prùmìrovaného perceptronu. Vstupními daty je korpus PDT 2.0 s údaji ve formátu CSTS. Podle dostupných údajù program dosahuje úspì¹nosti od 95 do 96 procent je tudí¾ nejpøesnìj¹ím znaèkovacím programem pro zpracování èeského jazyka. Z dùvodu pou¾ití stejných dat tedy nebude problém porovnat koneènou úspì¹nost námi pou¾ité implementace s úspì¹ností programu Morèe.

\chapter{Pou¾ité algoritmy}
Pro odli¹ení od existujících øe¹ení byl vybrán algoritmus Conditional Random Fields (Podmínìná náhodná pole), roz¹iøující Hidden Markov Model a tím nabízející vý¹¹í úèinnost na vstupních datech. Pro lep¹í pochopení CRF je potøeba nejdøíve porozumìt zpùsobu, jakým pracuje algoritmus HMM. V mnoha existujících øe¹eních se poté pro urèení parametrù pou¾ívají rùzné algoritmy od Viterbi algorithm, pou¾ívaného ji¾ od 60. let minulého století, pøes Forward-Backward algorithm, jeho modifikaci Baum-Welch algorithm a¾ po \uv{nejmodernìj¹í} zástupce, kterými jsou Low Memory BFGS a Stochastic Gradient Descent, poskytující nejlep¹í výsledky, nebo vysoké sní¾ení pamì»ové a èasové nároènosti.

Celý proces poté probíhá za tzv. Supervised Learning (uèení se pod dohledem), kdy programu nejdøíve pøedlo¾íme ji¾ správnì oznaèená data a necháme jej, aby se pomocí jednoho z vý¹e zmínìných algoritmù nauèil správné parametry modelu (CRF nebo HMM). Takto získaný model poté aplikujeme na men¹í skupinu dat, pomocí které poté urèíme koneènou úspì¹nost celého procesu.

\section{Hiden Markov Model (HMM)}
Skrytý Markovský model (viz.\cite{Kovar}), pojmenovaný po ruském matematikovi Andreyi Markovi, oznaèuje statistický model urèený pro popis posloupnosti obsahující Markovské stavy. Tìmi máme na mysli takový stav, pro nì¾ je dùle¾itá pouze blízká minulost (v na¹em pøípadì pouze nìkolik pøedchozích stavù -- znaèek) a vzdálená minulost je zanedbána. Ve zpracování jazyka se èasto omezuje zpracování pouze na dvì pøedchozí znaèky.

Zatímco u klasického Markovského modelu známe posloupnost vstupních hodnot, v na¹em pøípadì známe pouze posloupnost hodnot výsledných (slovních tvarù). V ka¾dém kroku je poté ulo¾ena hodnota pravdìpodobnosti pøechodu do mo¾ných následujících stavù a hodnota pravdìpodobnosti vygenerování urèitého výstupu (znaèky). Celý proces je zachycen na obrázku ~\ref{fig:GrafHMM}.

\begin{figure}[ht!]
	\centering
		\includegraphics[width=\textwidth]{fig/HMM}
		\caption{Graf mo¾ných pøechodù a výstupù modelu}
		\label{fig:GrafHMM}
\end{figure}

Z tohoto ukázkového pøíkladu mù¾eme vyvodit, ¾e pøi posloupnosti pozorovaných tvarù ètverec, kosoètverec, kruh, trojúhelník, mù¾e \uv{skrytá} vstupní posloupnost mít jednu z podob zobrazených v tabulce ~\ref{tab:HMMPrechody}.
\begin{figure}
	\begin{center}
		\begin{tabular}{|c c c c|}
			\hline
			1 & 2 & 1 & 1 \\
			\hline
			1 & 2 & 1 & 2 \\ 
			\hline
			1 & 2 & 2 & 1 \\ 
			\hline
			1 & 2 & 2 & 3 \\
			\hline
		\end{tabular}
		\captionof{table}{Mo¾né pøechody mezi skrytými stavy modelu}
		\label{tab:HMMPrechody}
	\end{center}
\end{figure}
Nejpravdìpodobnìj¹í z nich bychom byli schopni urèit pomocí ohodnocení jednotlivých pøechodù. 

V na¹em pøípadì bychom pak pøi trénování, kdy známe jak vstupní, tak výstupní posloupnost, museli urèitým zpùsobem vypoèítat hodnoty na jednotlivých pøechodech. K tomu mù¾eme pou¾ít jeden k tomu urèených algoritmù -- Viterbiho algoritmus, Forward-Backward algoritmus, nebo Baum-Welch algoritmus. S jejich pomocí jsme poté po nìkolika iteracích schopni získat hodnoty pro ideální prùchod pøi neznámé posloupnosti.

I kdy¾ není HMM implementováno v ¾ádném z vyu¾itých programù, poskytuje základ pro pochopení algoritmu CRF.

\section{Conditional Random Fields (CRF)}
CRF mù¾eme popsat jako bezsmìrový grafický pravdìpodobnostní model (undirected probabilistic graphical model), ve kterém ka¾dý uzel (v na¹em pøípadì slovo) popisuje náhodnou promìnnou. CRF mù¾e také být vyjádøen jako model s koneèným poètem stavù a nenormalizovanými pravdìpodobnostmi pøechodù. V porovnání k HMM nemá jasnì dané hodnoty pøechodù mezi stavy a disponuje mo¾ností mnohonásobných funkcí, generujících rysy. Pøiná¹í také lep¹í výsledky ve zpracování dat se závislostmi vy¹¹ího øádu, které vìt¹inou lépe odpovídají reálnému modelu. Na rozdíl od generativních modelù také, jako¾to pravdìpodobnostní (conditional) model nemusí zkoumat v¹echny mo¾né sekvence pozorovaných prvkù (observations). Pro urèení parametrù tohoto modelu je v souèasné dobì nejvíce doporuèován algoritmus L-BFGS (Low-Memory BFGS), který poskytuje nejkvalitnìj¹í výsledky.

\subsection{Label Bias problém}
Problém ¹patného vyva¾ování znaèek (viz.\cite{Lafferty}) spoèívá v pøesunu pravdìpodobnostní hodnoty na dal¹í pozici v pøípadì pouze jediného pøechodu za ignorování pozorovaného prvku. Jak je ilustrováno na obrázku DOPLNIT!!, v pøípadì vstupní posloupnosti písmen t,i,k se v prvním kroku neumíme rozhodnout a vstupujeme do stavù 1 a 4 se stejnou pravdìpodobností. Nyní, i kdy¾ na vstupu je písmeno i, nemáme jinou mo¾nost, ne¾ pokraèovat do stavù 2, pøípadnì 5 za ignorování pozorovaného písmena. V pøípadì, ¾e by v trénovacích datech tedy pøevládal øetìzec \uv{tik} nad øetìzcem \uv{tak}, byly by oba tyto øetìzce v¾dy urèeny jako \uv{tik}. Toto je vy¾e¹eno algoritmem CRF pomocí ohodnocení pøechodù mezi stavy, kdy mù¾eme podle aktuálního pozorovaného prvku buï sní¾it, nebo zvý¹it koneènou pravdìpodobnost dané posloupnosti.

\section{Viterbi algorithm}
Byl navrhnut v roce 1967 Andrewem Viterbi jako dekódovací algoritmus pro konvoluèní kódy v telekomunikaèních sítích. Kvùli svému zamìøení na vyhledání nejpravdìpodobnìj¹í cesty sekvencí skrytých stavù (nejèastìji HMM) se v¹ak pou¾ívá v mnoha odvìtvích, jako zpracování signálù a také znaèkování jazyka. 

Za pøedpokladu, ¾e vstupní data mají stejný poèet pozorovaných a skrytých stavù, obì posloupnosti jsou zarovnané a výpoèet v daném místì je závislý pouze na aktuálním a pøedchozím prvku, je pomocí získána tzv. Viterbiho cesta -- nejpravdìpodobnìj¹í cesta posloupností.  V ka¾dém kroku algoritmus vyhodnotí v¹echny cesty vedoucí k aktuálnímu stavu a ponechá pouze jednu, díky èemu¾ není nucen uchovávat v¹echny existující cesty, ale pouze jednu pro ka¾dý stav. Za pomocí uchovávaní ceny stavu se poka¾dé algoritmus rozhodne ze v¹ech mo¾ností pro dal¹í stav a ulo¾í kompletní cestu od zaèátku prùchodu posloupností.

\section{Forward-backward algorithm}
Pou¾ívá dva prùchody posloupností stavù -- dopøedu a zpìt (podle toho také vznikl jeho název). Nejdøíve projde celou sekvenci a urèí cenu cesty pøi daném prùchodu. Poté projde sekvencí zpìt a pøi ka¾dém kroku spoèítá pravdìpodobnost pozorování zbylých vstupních hodnot. Oba tyto prùchody jsou poté \uv{vyhlazeny} do spoleèného výsledku a tím je získáno rozdìlení mezi stavy v jednotlivých jednotkách èasu. Speciálním pøípadem je poté Baum-Welch algoritmus (ZJISTIT VICE??).

\section{L-BFGS}
L-BFGS (viz.\cite{Nash}) oznaèuje nízko pamì»ovou (Low memory) úpravu optimalizaèní metody BFGS (Broyden-Fletcher-Goldfarb-Shanno) z rodiny Quasi-Newtonových metod, urèenou k aproximaci inverzní Hessovy matice (ètvercová matice druhých parciálních derivací funkce -- popisuje místní zakøivení funkce o nìkolika promìnných). Optimalizací pro sní¾ení pamì»ové nároènosti je neukládání matice samotné, ale pouze ukládá historii zmìn.

\section{Stochastic Gradient Descent}

\section{Regularizace L1}

\section{Regularizace L2}

\chapter{Závìr}

%=========================================================================
